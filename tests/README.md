# Tests del Agente Conversacional IoT

Este directorio contiene la suite completa de tests para el proyecto del Agente Conversacional IoT.

## üìÅ Estructura de Tests

```
tests/
‚îú‚îÄ‚îÄ __init__.py                          # Informaci√≥n del m√≥dulo de tests
‚îú‚îÄ‚îÄ conftest.py                         # Configuraci√≥n global y fixtures
‚îú‚îÄ‚îÄ pytest.ini                         # Configuraci√≥n de pytest
‚îú‚îÄ‚îÄ README.md                          # Esta documentaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ üß™ Tests Unitarios
‚îú‚îÄ‚îÄ test_database.py                   # Tests de base de datos
‚îú‚îÄ‚îÄ test_agent.py                     # Tests del agente conversacional
‚îú‚îÄ‚îÄ test_tools.py                     # Tests de herramientas
‚îú‚îÄ‚îÄ test_system.py                    # Tests de integraci√≥n completos
‚îú‚îÄ‚îÄ test_simple.py                    # Tests b√°sicos y de conectividad
‚îú‚îÄ‚îÄ test_detail.py                    # Tests detallados de componentes
‚îú‚îÄ‚îÄ test_import.py                    # Tests de importaci√≥n de m√≥dulos
‚îú‚îÄ‚îÄ test_urls.py                      # Tests de URLs y conectividad
‚îú‚îÄ‚îÄ test_temperature_fix.py           # Tests de correcci√≥n de temperatura
‚îú‚îÄ‚îÄ test_streamlit_agent.py           # Tests espec√≠ficos del agente Streamlit
‚îÇ
‚îú‚îÄ‚îÄ üîç Tests de Diagn√≥stico
‚îú‚îÄ‚îÄ diagnose_api_access.py            # Diagn√≥stico exhaustivo de acceso a API
‚îú‚îÄ‚îÄ diagnostic_results.json           # Resultados del diagn√≥stico de API
‚îú‚îÄ‚îÄ streamlit_agent_test_results.json # Resultados del test del agente Streamlit
‚îÇ
‚îî‚îÄ‚îÄ üìä Tests de Integraci√≥n Completa
    ‚îú‚îÄ‚îÄ test_cloud_deployment.py      # Tests de despliegue en cloud
    ‚îú‚îÄ‚îÄ test_groq_integration.py      # Tests de integraci√≥n con Groq
    ‚îú‚îÄ‚îÄ test_jetson_api.py            # Tests de API de Jetson
    ‚îî‚îÄ‚îÄ ... (otros tests existentes)
```

## üöÄ C√≥mo Ejecutar los Tests

### Requisitos Previos

1. **Entorno virtual activado**:
   ```bash
   # Windows
   agente\Scripts\activate
   
   # Linux/Mac
   source agente/bin/activate
   ```

2. **Dependencias instaladas**:
   ```bash
   pip install pytest pytest-asyncio
   ```

### Comandos de Ejecuci√≥n

#### Ejecutar Todos los Tests
```bash
pytest
```

#### Ejecutar Tests Espec√≠ficos
```bash
# Por archivo
pytest tests/test_database.py
pytest tests/test_agent.py
pytest tests/test_tools.py

# Por funci√≥n espec√≠fica
pytest tests/test_database.py::test_database_connection
pytest tests/test_agent.py::test_agent_initialization
```

#### Ejecutar por Categor√≠as
```bash
# Solo tests unitarios
pytest -m unit

# Solo tests de integraci√≥n
pytest -m integration

# Tests que requieren base de datos
pytest -m database

# Tests del agente
pytest -m agent
```

#### Opciones √ötiles
```bash
# Modo verbose (m√°s detalle)
pytest -v

# Mostrar output de print statements
pytest -s

# Ejecutar tests en paralelo (requiere pytest-xdist)
pytest -n auto

# Detener en el primer fallo
pytest -x

# Generar reporte de cobertura
pytest --cov=modules --cov-report=html
```

## ÔøΩ Scripts de Diagn√≥stico

Los siguientes scripts proporcionan diagn√≥stico exhaustivo del sistema:

### diagnose_api_access.py
**Diagn√≥stico completo de acceso a la API de Jetson**

```bash
# Ejecutar diagn√≥stico exhaustivo
python tests/diagnose_api_access.py
```

Este script:
- ‚úÖ Prueba acceso directo a la API de Jetson
- ‚úÖ Verifica el conector del agente paso a paso  
- ‚úÖ Compara respuestas entre API directa vs conector
- ‚úÖ Identifica inconsistencias en dispositivos y datos
- ‚úÖ Genera reporte completo en `diagnostic_results.json`

**Casos de uso:**
- Cuando el frontend ve dispositivos que el agente no ve
- Problemas de conectividad con la Jetson
- Diferencias en cantidad de registros detectados
- Validaci√≥n de robustez del sistema

### test_streamlit_agent.py  
**Test espec√≠fico del agente de Streamlit**

```bash
# Probar agente de Streamlit completo
python tests/test_streamlit_agent.py
```

Este script:
- ‚úÖ Crea instancia real del CloudIoTAgent
- ‚úÖ Verifica health check completo
- ‚úÖ Ejecuta consultas espec√≠ficas (dispositivos, registros)
- ‚úÖ Valida acceso al conector interno
- ‚úÖ Genera reporte en `streamlit_agent_test_results.json`

**Casos de uso:**
- Validar funcionamiento del agente antes del deploy
- Verificar consultas espec√≠ficas por dispositivo
- Comprobar detecci√≥n inteligente de consultas
- Diagn√≥stico de respuestas del agente

### Archivos de Resultados

#### diagnostic_results.json
Contiene resultados completos del diagn√≥stico de API incluyendo:
- Pruebas directas a endpoints de Jetson
- Resultados del conector del agente  
- Comparaci√≥n de datos API vs agente
- Detalles de errores y conectividad

#### streamlit_agent_test_results.json
Contiene resultados del test del agente incluyendo:
- Health check del agente y componentes
- Respuestas a consultas espec√≠ficas
- Datos procesados y detectados
- Performance del conector interno

## ÔøΩüìã Tipos de Tests

### üîß Tests Unitarios (`test_*.py`)

- **test_database.py**: 
  - Conexi√≥n a base de datos
  - Operaciones CRUD
  - Manejo de errores
  - Health checks

- **test_agent.py**:
  - Inicializaci√≥n del agente
  - Procesamiento de mensajes
  - Determinaci√≥n de herramientas
  - Manejo de contexto

- **test_tools.py**:
  - Herramientas de base de datos
  - Herramientas de an√°lisis
  - Detecci√≥n de anomal√≠as
  - Generaci√≥n de reportes

### üîó Tests de Integraci√≥n (`test_system.py`)

- Funcionamiento completo del sistema
- Integraci√≥n entre componentes
- Flujos de trabajo end-to-end
- Demostraciones interactivas

## üè∑Ô∏è Marcadores de Tests

Los tests est√°n categorizados con marcadores para facilitar la ejecuci√≥n selectiva:

- `@pytest.mark.unit`: Tests unitarios b√°sicos
- `@pytest.mark.integration`: Tests de integraci√≥n
- `@pytest.mark.slow`: Tests que toman m√°s tiempo
- `@pytest.mark.database`: Tests que requieren DB
- `@pytest.mark.agent`: Tests espec√≠ficos del agente
- `@pytest.mark.tools`: Tests de herramientas

### Ejemplos de Uso
```bash
# Solo tests r√°pidos (excluir lentos)
pytest -m "not slow"

# Tests de base de datos √∫nicamente
pytest -m database

# Combinaciones
pytest -m "unit and not slow"
pytest -m "agent or tools"
```

## üìä Fixtures Disponibles

Las fixtures est√°n definidas en `conftest.py` y est√°n disponibles para todos los tests:

- `sample_device_data`: Datos de dispositivos de ejemplo
- `sample_sensor_data`: Datos de sensores de ejemplo  
- `sample_alert_data`: Datos de alertas de ejemplo
- `setup_test_environment`: Configuraci√≥n autom√°tica del entorno

### Usar Fixtures en Tests
```python
def test_example(sample_sensor_data):
    # sample_sensor_data est√° disponible autom√°ticamente
    assert len(sample_sensor_data) > 0
```

## üêõ Depuraci√≥n de Tests

### Ver Output Detallado
```bash
# Mostrar prints y logs
pytest -s -v

# Mostrar traceback completo
pytest --tb=long

# Debugger interactivo en fallos
pytest --pdb
```

### Tests Espec√≠ficos con Filtros
```bash
# Por nombre de funci√≥n
pytest -k "test_database"

# Por nombre de clase
pytest -k "TestDatabaseTools"

# Expresiones complejas
pytest -k "database and not slow"
```

## üìà Cobertura de C√≥digo

Para generar reportes de cobertura:

```bash
# Instalar pytest-cov
pip install pytest-cov

# Generar reporte en terminal
pytest --cov=modules

# Generar reporte HTML
pytest --cov=modules --cov-report=html

# Ver reporte en navegador
open htmlcov/index.html  # Mac/Linux
start htmlcov/index.html # Windows
```

## üîß Resoluci√≥n de Problemas Espec√≠ficos

### Problema: Agente no ve todos los dispositivos
**S√≠ntomas**: El frontend muestra N dispositivos pero el agente solo ve M dispositivos

```bash
# 1. Diagnosticar acceso a API
python tests/diagnose_api_access.py

# 2. Verificar agente espec√≠ficamente  
python tests/test_streamlit_agent.py

# 3. Revisar logs para identificar el problema
```

**Puntos de verificaci√≥n:**
- ‚úÖ API de Jetson responde con todos los dispositivos
- ‚úÖ Conector del agente recibe todos los dispositivos
- ‚úÖ Procesamiento de datos no filtra dispositivos incorrectamente
- ‚úÖ Consultas espec√≠ficas distribuyen datos por dispositivo

### Problema: Consultas "√∫ltimos X registros" incorrectas
**S√≠ntomas**: Al pedir "√∫ltimos 10 registros de cada dispositivo" solo muestra de uno

```bash
# Verificar l√≥gica de consultas espec√≠ficas
python tests/test_streamlit_agent.py
```

**Soluci√≥n implementada:**
- ‚úÖ Detecci√≥n de "por dispositivo" vs "total"
- ‚úÖ Distribuci√≥n equitativa de registros
- ‚úÖ Formato mejorado agrupando por dispositivo

### Problema: Conectividad robusta
**S√≠ntomas**: Sistema falla tras cortes de energ√≠a o cambios de URL

```bash
# Verificar sistema robusto
python tests/diagnose_api_access.py
```

**Caracter√≠sticas verificadas:**
- ‚úÖ Auto-detecci√≥n de URLs funcionales
- ‚úÖ Reconexi√≥n autom√°tica
- ‚úÖ Reintentos inteligentes
- ‚úÖ Fallback graceful

## üîß Configuraci√≥n Personalizada

### Variables de Entorno para Tests

Los tests usan variables de entorno mockeadas definidas en `conftest.py`. Para tests que requieren configuraci√≥n real:

```bash
# Crear archivo .env.test
DB_HOST=test_host
DB_PORT=5432
DB_NAME=test_iot_db
OPENAI_API_KEY=test_key
```

### Configuraci√≥n de pytest.ini

El archivo `pytest.ini` contiene la configuraci√≥n predeterminada. Puedes modificarla seg√∫n tus necesidades:

```ini
[tool:pytest]
addopts = -v --tb=short --strict-markers
testpaths = tests
markers = 
    unit: Unit tests
    integration: Integration tests
```

## üö® Resoluci√≥n de Problemas

### Errores Comunes

1. **ImportError de m√≥dulos**:
   ```bash
   # Aseg√∫rate de estar en el directorio ra√≠z
   cd /path/to/agent_LangGreph
   pytest
   ```

2. **Tests de base de datos fallan**:
   ```bash
   # Verificar configuraci√≥n de DB en .env
   # O ejecutar solo tests unitarios
   pytest -m "not database"
   ```

3. **Tests asyncio fallan**:
   ```bash
   # Instalar pytest-asyncio
   pip install pytest-asyncio
   ```

### Logs y Debugging

```python
# Agregar logging en tests
import logging
logging.basicConfig(level=logging.DEBUG)

def test_with_logs():
    logger = logging.getLogger(__name__)
    logger.debug("Debug message in test")
```

## üìù Contribuir con Tests

### Agregar Nuevos Tests

1. **Crear archivo de test**: `test_nuevo_modulo.py`
2. **Usar convenciones**: Nombres empiezan con `test_`
3. **Agregar marcadores**: `@pytest.mark.unit`
4. **Documentar**: Docstrings explicando qu√© prueba cada test
5. **Usar fixtures**: Reutilizar datos de ejemplo

### Ejemplo de Test Nuevo
```python
import pytest

@pytest.mark.unit
def test_nueva_funcionalidad(sample_data):
    """Test para verificar nueva funcionalidad."""
    # Arrange
    input_data = sample_data
    
    # Act
    result = nueva_funcion(input_data)
    
    # Assert
    assert result is not None
    assert isinstance(result, dict)
```

---

Para m√°s informaci√≥n sobre testing en Python, consulta la [documentaci√≥n de pytest](https://docs.pytest.org/).
