#!/usr/bin/env python3
"""
Test del Sistema de Seguimiento de Uso de API
============================================

Prueba completa del sistema de contadores de consultas y tokens.
"""

import os
import sys
import json
from datetime import datetime, date
from pathlib import Path

# Agregar path del proyecto
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from modules.utils.usage_tracker import UsageTracker, usage_tracker
from modules.agents.cloud_iot_agent import CloudIoTAgent
import asyncio

def test_usage_tracker_basic():
    """Test bÃ¡sico del sistema de seguimiento"""
    print("ğŸ§ª INICIANDO TESTS DEL SISTEMA DE SEGUIMIENTO")
    print("=" * 60)
    
    try:
        # 1. Test de inicializaciÃ³n
        print("\n1ï¸âƒ£ Test de InicializaciÃ³n")
        tracker = UsageTracker("test_usage.json")
        print(f"   âœ… Tracker inicializado")
        print(f"   ğŸ“… Fecha actual: {tracker.usage_data['last_reset_date']}")
        
        # 2. Test de informaciÃ³n de modelo
        print("\n2ï¸âƒ£ Test de InformaciÃ³n de Modelo")
        model = "llama-3.1-8b-instant"
        usage_info = tracker.get_usage_info(model)
        
        print(f"   ğŸ¤– Modelo: {usage_info['model_description']}")
        print(f"   ğŸ”¥ LÃ­mite requests: {usage_info['requests_limit']:,}")
        print(f"   ğŸ¯ LÃ­mite tokens: {usage_info['tokens_limit']:,}")
        print(f"   ğŸ“Š Estado: {usage_info['status']}")
        
        # 3. Test de registro de consultas
        print("\n3ï¸âƒ£ Test de Registro de Consultas")
        
        # Simular varias consultas
        for i in range(5):
            tokens_used = 100 + (i * 50)  # Variar tokens
            result = tracker.track_request(model, tokens_used)
            
            print(f"   Consulta {i+1}: {result['requests_used']} requests, {result['tokens_used']} tokens")
        
        # 4. Test de verificaciÃ³n de lÃ­mites
        print("\n4ï¸âƒ£ Test de VerificaciÃ³n de LÃ­mites")
        can_make_request, message = tracker.check_can_make_request(model)
        print(f"   âœ… Puede hacer consulta: {can_make_request}")
        print(f"   ğŸ’¬ Mensaje: {message}")
        
        # 5. Test de resumen diario
        print("\n5ï¸âƒ£ Test de Resumen Diario")
        summary = tracker.get_daily_summary()
        print(f"   ğŸ“Š Total requests hoy: {summary['total_requests_today']}")
        print(f"   ğŸ¯ Total tokens hoy: {summary['total_tokens_today']}")
        print(f"   ğŸ¤– Modelos usados: {summary['models_used_today']}")
        
        # 6. Test de mÃºltiples modelos
        print("\n6ï¸âƒ£ Test de MÃºltiples Modelos")
        other_model = "llama-3.1-70b-versatile"
        tracker.track_request(other_model, 200)
        
        all_usage = tracker.get_all_models_usage()
        print(f"   ğŸ“ˆ Modelos con datos: {len(all_usage)}")
        
        for model_id, info in all_usage.items():
            if info["requests_used"] > 0:
                print(f"   â€¢ {info['model_description']}: {info['requests_used']} requests")
        
        print("\nâœ… TODOS LOS TESTS BÃSICOS COMPLETADOS")
        
        # Limpiar archivo de test
        test_file = Path("test_usage.json")
        if test_file.exists():
            test_file.unlink()
            print("ğŸ§¹ Archivo de test limpiado")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en tests bÃ¡sicos: {e}")
        return False

async def test_usage_integration():
    """Test de integraciÃ³n con CloudIoTAgent"""
    print("\nğŸ”— INICIANDO TEST DE INTEGRACIÃ“N")
    print("=" * 50)
    
    try:
        # Test con agente real
        print("\n1ï¸âƒ£ Creando CloudIoTAgent")
        agent = CloudIoTAgent()
        
        print("\n2ï¸âƒ£ Inicializando agente")
        success = await agent.initialize()
        print(f"   âœ… InicializaciÃ³n: {success}")
        
        print("\n3ï¸âƒ£ Verificando health check con uso de API")
        health = await agent.health_check()
        
        if "api_usage" in health:
            usage = health["api_usage"]
            print(f"   ğŸ¤– Modelo: {usage['model']}")
            print(f"   ğŸ”¥ Requests: {usage['requests_used']}/{usage['requests_limit']}")
            print(f"   ğŸ¯ Tokens: {usage['tokens_used']}/{usage['tokens_limit']}")
            print(f"   ğŸ“Š Estado: {usage['status']}")
            print("   âœ… InformaciÃ³n de uso incluida en health check")
        else:
            print("   âš ï¸ InformaciÃ³n de uso no encontrada en health check")
        
        print("\n4ï¸âƒ£ Procesando consulta de prueba")
        response = await agent.process_query("Â¿CuÃ¡l es la temperatura actual?")
        
        if response.get("success"):
            print("   âœ… Consulta procesada exitosamente")
            
            # Verificar si hay informaciÃ³n de uso en la respuesta
            if "usage_info" in response:
                usage = response["usage_info"]
                print(f"   ğŸ“Š Uso actualizado: {usage['requests_used']} requests")
            else:
                print("   â„¹ï¸ InformaciÃ³n de uso no en respuesta directa")
                
        else:
            print(f"   âš ï¸ Consulta fallÃ³: {response.get('error', 'Error desconocido')}")
        
        print("\nâœ… TEST DE INTEGRACIÃ“N COMPLETADO")
        return True
        
    except Exception as e:
        print(f"âŒ Error en test de integraciÃ³n: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_usage_limits():
    """Test de comportamiento cerca de lÃ­mites"""
    print("\nâš ï¸ INICIANDO TEST DE LÃMITES")
    print("=" * 40)
    
    try:
        # Crear tracker de prueba con lÃ­mites bajos
        tracker = UsageTracker("test_limits.json")
        
        # Simular modelo con lÃ­mite bajo para testing
        model = "test-model"
        tracker.daily_limits[model] = {
            "requests": 10,  # LÃ­mite muy bajo para testing
            "tokens": 1000,
            "description": "Modelo de Test"
        }
        
        print(f"\n1ï¸âƒ£ Simulando uso hasta lÃ­mite (10 requests)")
        
        # Usar hasta el lÃ­mite
        for i in range(12):  # Intentar 12, lÃ­mite en 10
            can_make, message = tracker.check_can_make_request(model)
            
            if can_make:
                result = tracker.track_request(model, 50)
                status_emoji = {
                    "normal": "âœ…",
                    "warning": "âš ï¸",
                    "critical": "ğŸš¨"
                }.get(result["status"], "â“")
                
                print(f"   Request {i+1}: {status_emoji} {result['requests_used']}/10 - {result['status']}")
            else:
                print(f"   Request {i+1}: ğŸš« RECHAZADO - {message}")
        
        print("\n2ï¸âƒ£ Verificando estado final")
        final_info = tracker.get_usage_info(model)
        print(f"   ğŸ“Š Requests finales: {final_info['requests_used']}/{final_info['requests_limit']}")
        print(f"   ğŸ¯ Estado final: {final_info['status']}")
        print(f"   âœ… Puede hacer mÃ¡s: {final_info['can_make_request']}")
        
        # Limpiar
        test_file = Path("test_limits.json")
        if test_file.exists():
            test_file.unlink()
        
        print("\nâœ… TEST DE LÃMITES COMPLETADO")
        return True
        
    except Exception as e:
        print(f"âŒ Error en test de lÃ­mites: {e}")
        return False

def main():
    """Ejecutar todos los tests"""
    print("ğŸš€ INICIANDO SUITE COMPLETA DE TESTS")
    print("=" * 70)
    
    results = []
    
    # Test 1: BÃ¡sico
    print("\n" + "="*70)
    result1 = test_usage_tracker_basic()
    results.append(("Test BÃ¡sico", result1))
    
    # Test 2: IntegraciÃ³n  
    print("\n" + "="*70)
    result2 = asyncio.run(test_usage_integration())
    results.append(("Test IntegraciÃ³n", result2))
    
    # Test 3: LÃ­mites
    print("\n" + "="*70)
    result3 = test_usage_limits()
    results.append(("Test LÃ­mites", result3))
    
    # Resumen final
    print("\n" + "="*70)
    print("ğŸ“‹ RESUMEN DE TESTS")
    print("=" * 30)
    
    total_tests = len(results)
    passed_tests = sum(1 for _, passed in results if passed)
    
    for test_name, passed in results:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"   {status} {test_name}")
    
    print(f"\nğŸ“Š RESULTADO FINAL: {passed_tests}/{total_tests} tests pasaron")
    
    if passed_tests == total_tests:
        print("ğŸ‰ Â¡TODOS LOS TESTS COMPLETADOS EXITOSAMENTE!")
        print("\nğŸ’¡ El sistema de seguimiento de uso estÃ¡ listo para producciÃ³n")
        print("   â€¢ Contadores funcionando correctamente")
        print("   â€¢ LÃ­mites respetados")
        print("   â€¢ IntegraciÃ³n con CloudIoTAgent exitosa")
        print("   â€¢ InformaciÃ³n de uso disponible en health checks")
    else:
        print("âš ï¸ Algunos tests fallaron. Revisar implementaciÃ³n.")
    
    return passed_tests == total_tests

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)